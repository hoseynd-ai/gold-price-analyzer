#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NewsAPI Service for Historical News Collection

Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± ØªØ§Ø±ÛŒØ®ÛŒ Ø·Ù„Ø§ Ø§Ø² NewsAPI.org

Author: Hoseyn Doulabi (@hoseynd-ai)
Created: 2025-10-25 16:09:04 UTC
"""

import requests
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from sqlalchemy import select

from app.infrastructure.database.base import AsyncSessionLocal
from app.infrastructure.database.models import NewsEvent
from app.core.logging import get_logger
from app.core.config import settings

logger = get_logger(__name__)


class NewsAPIService:
    """
    Ø³Ø±ÙˆÛŒØ³ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± ØªØ§Ø±ÛŒØ®ÛŒ Ø§Ø² NewsAPI
    
    Features:
    - Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ø§Ø² 1 Ù…Ø§Ù‡ Ú¯Ø°Ø´ØªÙ‡ (free tier)
    - ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø§Ø®Ø¨Ø§Ø± Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø·Ù„Ø§
    - Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± database
    - Ø­Ø°Ù duplicate Ù‡Ø§
    
    Rate Limits (Free):
    - 100 requests/day
    - 1000 requests/month
    - News from last 1 month only
    """
    
    BASE_URL = "https://newsapi.org/v2/everything"
    
    # Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø·Ù„Ø§ (Ø¨Ù‡ØªØ±ÛŒÙ†â€ŒÙ‡Ø§)
    GOLD_KEYWORDS = [
        "gold price",
        "gold market",
        "gold trading",
        "precious metals gold",
        "gold investment",
        "gold bullion",
        "gold futures",
        "spot gold",
    ]
    
    def __init__(self, api_key: Optional[str] = None):
        """
        Initialize NewsAPI service
        
        Args:
            api_key: NewsAPI key (Ø§Ø² .env ÛŒØ§ manual)
        """
        self.api_key = api_key or settings.NEWSAPI_KEY
        
        if not self.api_key:
            logger.error("newsapi_key_not_configured")
            raise ValueError("NewsAPI key not found in .env")
        
        logger.info("newsapi_service_initialized")
    
    async def fetch_news(
        self,
        keyword: str,
        from_date: datetime,
        to_date: datetime,
        language: str = 'en',
        page_size: int = 100
    ) -> List[Dict]:
        """
        Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ø¨Ø§ ÛŒÚ© keyword
        
        Args:
            keyword: Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ
            from_date: Ø§Ø² ØªØ§Ø±ÛŒØ®
            to_date: ØªØ§ ØªØ§Ø±ÛŒØ®
            language: Ø²Ø¨Ø§Ù†
            page_size: ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø± Ù‡Ø± ØµÙØ­Ù‡ (max: 100)
            
        Returns:
            Ù„ÛŒØ³Øª Ø§Ø®Ø¨Ø§Ø±
        """
        logger.info("fetching_news",
                   keyword=keyword,
                   from_date=from_date.date(),
                   to_date=to_date.date())
        
        params = {
            'q': keyword,
            'from': from_date.strftime('%Y-%m-%d'),
            'to': to_date.strftime('%Y-%m-%d'),
            'language': language,
            'sortBy': 'relevancy',  # ÛŒØ§ 'publishedAt' ÛŒØ§ 'popularity'
            'apiKey': self.api_key,
            'pageSize': page_size
        }
        
        try:
            response = requests.get(self.BASE_URL, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            if data.get('status') == 'ok':
                articles = data.get('articles', [])
                logger.info("news_fetched",
                           keyword=keyword,
                           count=len(articles),
                           total_results=data.get('totalResults', 0))
                return articles
            else:
                error_msg = data.get('message', 'Unknown error')
                logger.error("newsapi_error",
                           keyword=keyword,
                           error=error_msg)
                return []
                
        except requests.exceptions.RequestException as e:
            logger.error("request_error", keyword=keyword, error=str(e))
            return []
    
    async def fetch_historical_news(
        self,
        days_back: int = 30,
        keywords: Optional[List[str]] = None
    ) -> int:
        """
        Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± ØªØ§Ø±ÛŒØ®ÛŒ
        
        Args:
            days_back: Ú†Ù†Ø¯ Ø±ÙˆØ² Ø¹Ù‚Ø¨ (max: 30 for free tier)
            keywords: Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: GOLD_KEYWORDS)
            
        Returns:
            ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø± Ø¬Ø¯ÛŒØ¯ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡
        """
        # Free tier ÙÙ‚Ø· 1 Ù…Ø§Ù‡ Ø§Ø®ÛŒØ±
        if days_back > 30:
            logger.warning("free_tier_limit",
                          requested=days_back,
                          limited_to=30)
            days_back = 30
        
        to_date = datetime.utcnow()
        from_date = to_date - timedelta(days=days_back)
        
        logger.info("starting_historical_fetch",
                   from_date=from_date.date(),
                   to_date=to_date.date(),
                   days=days_back)
        
        keywords_to_use = keywords or self.GOLD_KEYWORDS
        all_articles = []
        
        # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¨Ø§ Ù‡Ø± keyword
        for i, keyword in enumerate(keywords_to_use, 1):
            print(f"ðŸ“° [{i}/{len(keywords_to_use)}] Searching: '{keyword}'...")
            
            articles = await self.fetch_news(
                keyword=keyword,
                from_date=from_date,
                to_date=to_date
            )
            
            all_articles.extend(articles)
            print(f"   âœ… Found: {len(articles)} articles")
            
            # Ú©Ù…ÛŒ ØµØ¨Ø± Ú©Ù†ÛŒÙ… (rate limit)
            if i < len(keywords_to_use):
                import asyncio
                await asyncio.sleep(1)
        
        # Ø­Ø°Ù duplicate
        print(f"\nðŸ”„ Removing duplicates...")
        unique_articles = self._deduplicate_articles(all_articles)
        print(f"   âœ… Unique articles: {len(unique_articles)}")
        
        # Ø°Ø®ÛŒØ±Ù‡
        print(f"\nðŸ’¾ Saving to database...")
        saved = await self._save_articles(unique_articles)
        
        logger.info("historical_fetch_complete",
                   total_fetched=len(all_articles),
                   unique=len(unique_articles),
                   saved=saved)
        
        return saved
    
    def _deduplicate_articles(self, articles: List[Dict]) -> List[Dict]:
        """
        Ø­Ø°Ù Ø§Ø®Ø¨Ø§Ø± ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ URL
        
        Args:
            articles: Ù„ÛŒØ³Øª Ø§Ø®Ø¨Ø§Ø±
            
        Returns:
            Ù„ÛŒØ³Øª Ø¨Ø¯ÙˆÙ† ØªÚ©Ø±Ø§Ø±
        """
        seen_urls = set()
        seen_titles = set()
        unique = []
        
        for article in articles:
            url = article.get('url', '')
            title = article.get('title', '')
            
            # Ú†Ú© URL
            if url and url not in seen_urls:
                seen_urls.add(url)
                
                # Ú†Ú© title (Ø¨Ø±Ø§ÛŒ Ø§Ø®Ø¨Ø§Ø±ÛŒ Ú©Ù‡ URL Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¯Ø§Ø±Ù†)
                if title and title not in seen_titles:
                    seen_titles.add(title)
                    unique.append(article)
        
        return unique
    
    async def _save_articles(self, articles: List[Dict]) -> int:
        """
        Ø°Ø®ÛŒØ±Ù‡ Ø§Ø®Ø¨Ø§Ø± Ø¯Ø± database
        
        Args:
            articles: Ù„ÛŒØ³Øª Ø§Ø®Ø¨Ø§Ø±
            
        Returns:
            ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø± Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡
        """
        saved_count = 0
        skipped_count = 0
        
        async with AsyncSessionLocal() as session:
            for article in articles:
                try:
                    url = article.get('url')
                    title = article.get('title')
                    
                    if not url or not title:
                        continue
                    
                    # Ú†Ú© ÙˆØ¬ÙˆØ¯ Ø¯Ø± database
                    result = await session.execute(
                        select(NewsEvent).where(NewsEvent.url == url)
                    )
                    existing = result.scalar_one_or_none()
                    
                    if existing:
                        skipped_count += 1
                        continue
                    
                    # Parse ØªØ§Ø±ÛŒØ®
                    published_at_str = article.get('publishedAt', '')
                    try:
                        published_at = datetime.fromisoformat(
                            published_at_str.replace('Z', '+00:00')
                        )
                    except:
                        published_at = datetime.utcnow()
                    
                    # Ø³Ø§Ø®Øª NewsEvent
                    news_event = NewsEvent(
                        title=title[:500],
                        description=article.get('description', '')[:2000] or '',
                        url=url[:500],
                        source='newsapi',
                        author=article.get('author', 'Unknown')[:200] or 'Unknown',
                        published_at=published_at,
                        category='market',
                        sentiment_score=None,  # Ø¨Ø¹Ø¯Ø§Ù‹ Ø¨Ø§ FinBERT
                        sentiment_label=None,
                        confidence=None
                    )
                    
                    session.add(news_event)
                    saved_count += 1
                    
                except Exception as e:
                    logger.warning("save_article_error",
                                 title=article.get('title', 'N/A')[:50],
                                 error=str(e))
                    continue
            
            await session.commit()
        
        logger.info("articles_saved",
                   saved=saved_count,
                   skipped=skipped_count)
        
        return saved_count


if __name__ == "__main__":
    import asyncio
    
    async def test():
        print("\n" + "="*70)
        print("ðŸ§ª Testing NewsAPI Service")
        print("="*70 + "\n")
        
        service = NewsAPIService()
        
        # ØªØ³Øª Ø¨Ø§ 7 Ø±ÙˆØ² Ú¯Ø°Ø´ØªÙ‡
        saved = await service.fetch_historical_news(days_back=7)
        
        print(f"\nâœ… Test complete! Saved {saved} articles\n")
    
    asyncio.run(test())
