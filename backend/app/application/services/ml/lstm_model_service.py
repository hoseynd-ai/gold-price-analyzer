#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Gold Price Analyzer - LSTM Model Service

Ù…Ø¯Ù„ LSTM Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù‚ÛŒÙ…Øª Ø·Ù„Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³:
- Ù‚ÛŒÙ…Øªâ€ŒÙ‡Ø§ÛŒ Ú¯Ø°Ø´ØªÙ‡ (OHLCV)
- Ø§Ù†Ø¯ÛŒÚ©Ø§ØªÙˆØ±Ù‡Ø§ÛŒ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ (RSI, MACD, BB)
- Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ø®Ø¨Ø§Ø± (FinBERT sentiment)

Author: Hoseyn Doulabi (@hoseynd-ai)
Created: 2025-10-25 15:46:25 UTC
"""

import numpy as np
import pandas as pd
from datetime import datetime
from typing import Tuple, Dict, Any, Optional
import joblib
import json
import os

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam

from app.core.logging import get_logger
from app.application.services.ml.feature_engineering_service import FeatureEngineeringService

logger = get_logger(__name__)


class LSTMGoldPricePredictor:
    """
    Ù…Ø¯Ù„ LSTM Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù‚ÛŒÙ…Øª Ø·Ù„Ø§
    
    Ø§ÛŒÙ† Ù…Ø¯Ù„:
    1. Ø§Ø² 42 feature Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    2. Ø¨Ø§ 21 Ø³Ø§Ù„ Ø¯Ø§Ø¯Ù‡ train Ù…ÛŒâ€ŒØ´ÙˆØ¯
    3. Ù‚ÛŒÙ…Øª 1, 7, 30 Ø±ÙˆØ² Ø¢ÛŒÙ†Ø¯Ù‡ Ø±Ø§ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    
    Author: Hoseyn Doulabi (@hoseynd-ai)
    Created: 2025-10-25 15:46:25 UTC
    """
    
    def __init__(
        self,
        sequence_length: int = 60,
        prediction_horizon: int = 1,
        lstm_units: list = [128, 64, 32],
        dropout_rate: float = 0.2
    ):
        """
        Initialize LSTM model
        
        Args:
            sequence_length: Ú†Ù†Ø¯ Ø±ÙˆØ² Ú¯Ø°Ø´ØªÙ‡ Ø±Ùˆ Ø¨Ø¨ÛŒÙ†Ù‡ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶: 60 Ø±ÙˆØ²)
            prediction_horizon: Ú†Ù†Ø¯ Ø±ÙˆØ² Ø¢ÛŒÙ†Ø¯Ù‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù†Ù‡ (1, 7, 30)
            lstm_units: ØªØ¹Ø¯Ø§Ø¯ units Ø¯Ø± Ù‡Ø± Ù„Ø§ÛŒÙ‡ LSTM
            dropout_rate: Ù†Ø±Ø® dropout Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² overfitting
        """
        self.sequence_length = sequence_length
        self.prediction_horizon = prediction_horizon
        self.lstm_units = lstm_units
        self.dropout_rate = dropout_rate
        
        self.model = None
        self.scaler_X = MinMaxScaler(feature_range=(0, 1))
        self.scaler_y = MinMaxScaler(feature_range=(0, 1))
        
        self.feature_names = None
        self.training_history = None
        self.metrics = {}
        
        logger.info("lstm_model_initialized",
                   sequence_length=sequence_length,
                   prediction_horizon=prediction_horizon,
                   lstm_units=lstm_units)
    
    def create_sequences(
        self, 
        X: np.ndarray, 
        y: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Ø³Ø§Ø®Øª sequences Ø¨Ø±Ø§ÛŒ LSTM
        
        LSTM Ù†ÛŒØ§Ø² Ø¯Ø§Ø±Ù‡ Ú©Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª ØªÙˆØ§Ù„ÛŒ (sequence) Ø¨Ø§Ø´Ù†.
        Ù…Ø«Ù„Ø§Ù‹: Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù‚ÛŒÙ…Øª ÙØ±Ø¯Ø§ØŒ 60 Ø±ÙˆØ² Ú¯Ø°Ø´ØªÙ‡ Ø±Ùˆ Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ù‡.
        
        Args:
            X: Features (5197, 42)
            y: Target (5197, 1)
            
        Returns:
            X_seq: (samples, sequence_length, features)
            y_seq: (samples, 1)
        """
        logger.info("creating_sequences", 
                   X_shape=X.shape,
                   sequence_length=self.sequence_length)
        
        X_seq = []
        y_seq = []
        
        for i in range(self.sequence_length, len(X)):
            # Ø¢Ø®Ø±ÛŒÙ† sequence_length Ø±ÙˆØ²
            X_seq.append(X[i - self.sequence_length:i])
            # Ù‚ÛŒÙ…Øª target
            y_seq.append(y[i])
        
        X_seq = np.array(X_seq)
        y_seq = np.array(y_seq)
        
        logger.info("sequences_created",
                   X_seq_shape=X_seq.shape,
                   y_seq_shape=y_seq.shape)
        
        return X_seq, y_seq
    
    def build_model(self, input_shape: Tuple[int, int]) -> keras.Model:
        """
        Ø³Ø§Ø®Øª Ù…Ø¹Ù…Ø§Ø±ÛŒ LSTM
        
        Ù…Ø¹Ù…Ø§Ø±ÛŒ:
        1. Bidirectional LSTM (Ù…ÛŒâ€ŒØªÙˆÙ†Ù‡ Ø§Ø² Ø¯Ùˆ Ø·Ø±Ù Ø¨Ø¨ÛŒÙ†Ù‡)
        2. Dropout layers (Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² overfitting)
        3. Dense layers (fully connected)
        
        Args:
            input_shape: (sequence_length, n_features)
            
        Returns:
            Compiled Keras model
        """
        logger.info("building_model", input_shape=input_shape)
        
        model = Sequential([
            # Ù„Ø§ÛŒÙ‡ Ø§ÙˆÙ„: Bidirectional LSTM
            Bidirectional(
                LSTM(
                    self.lstm_units[0],
                    return_sequences=True
                ),
                input_shape=input_shape
            ),
            Dropout(self.dropout_rate),
            
            # Ù„Ø§ÛŒÙ‡ Ø¯ÙˆÙ…: LSTM
            LSTM(self.lstm_units[1], return_sequences=True),
            Dropout(self.dropout_rate),
            
            # Ù„Ø§ÛŒÙ‡ Ø³ÙˆÙ…: LSTM
            LSTM(self.lstm_units[2], return_sequences=False),
            Dropout(self.dropout_rate),
            
            # Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Dense
            Dense(32, activation='relu'),
            Dropout(self.dropout_rate),
            
            Dense(16, activation='relu'),
            
            # Ø®Ø±ÙˆØ¬ÛŒ: 1 Ø¹Ø¯Ø¯ (Ù‚ÛŒÙ…Øª Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø´Ø¯Ù‡)
            Dense(1)
        ])
        
        # Build model Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ params
        model.build(input_shape=(None,) + input_shape)
        
        # Compile
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )
        
        logger.info("model_built", 
                   total_params=model.count_params(),
                   layers=len(model.layers))
        
        return model
    
    def train(
        self,
        X: pd.DataFrame,
        y: pd.DataFrame,
        validation_split: float = 0.2,
        epochs: int = 100,
        batch_size: int = 32
    ) -> Dict[str, Any]:
        """
        Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ LSTM
        
        Args:
            X: Features DataFrame
            y: Target DataFrame
            validation_split: Ø¯Ø±ØµØ¯ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ validation
            epochs: ØªØ¹Ø¯Ø§Ø¯ epochs
            batch_size: Ø§Ù†Ø¯Ø§Ø²Ù‡ batch
            
        Returns:
            Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø´Ø§Ù…Ù„ metrics Ùˆ history
        """
        logger.info("starting_training",
                   X_shape=X.shape,
                   y_shape=y.shape,
                   epochs=epochs)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†Ø§Ù… features
        self.feature_names = X.columns.tolist()
        
        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ numpy
        X_array = X.values
        y_array = y.values
        
        # Scaling
        logger.info("scaling_data")
        X_scaled = self.scaler_X.fit_transform(X_array)
        y_scaled = self.scaler_y.fit_transform(y_array)
        
        # Ø³Ø§Ø®Øª sequences
        X_seq, y_seq = self.create_sequences(X_scaled, y_scaled)
        
        # Split train/validation
        split_idx = int(len(X_seq) * (1 - validation_split))
        X_train = X_seq[:split_idx]
        y_train = y_seq[:split_idx]
        X_val = X_seq[split_idx:]
        y_val = y_seq[split_idx:]
        
        logger.info("data_split",
                   train_samples=len(X_train),
                   val_samples=len(X_val))
        
        # Ø³Ø§Ø®Øª model
        input_shape = (X_train.shape[1], X_train.shape[2])
        self.model = self.build_model(input_shape)
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ models
        os.makedirs('models', exist_ok=True)
        
        # Callbacks
        callbacks = [
            # Early stopping Ø§Ú¯Ø± val_loss Ø¨Ù‡Ø¨ÙˆØ¯ Ù†ÛŒØ§ÙØª
            EarlyStopping(
                monitor='val_loss',
                patience=15,
                restore_best_weights=True,
                verbose=1
            ),
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ† model
            ModelCheckpoint(
                'models/lstm_best_model.h5',
                monitor='val_loss',
                save_best_only=True,
                verbose=1
            ),
            
            # Ú©Ø§Ù‡Ø´ learning rate
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=0.00001,
                verbose=1
            )
        ]
        
        # Training
        logger.info("training_started")
        history = self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        self.training_history = history.history
        logger.info("training_completed",
                   final_loss=history.history['loss'][-1],
                   final_val_loss=history.history['val_loss'][-1])
        
        # Evaluation
        metrics = self.evaluate(X_val, y_val)
        
        return {
            'history': history.history,
            'metrics': metrics,
            'train_samples': len(X_train),
            'val_samples': len(X_val)
        }
    
    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:
        """
        Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„
        
        Args:
            X: Features
            y: True values
            
        Returns:
            Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ metrics
        """
        logger.info("evaluating_model")
        
        # Scaling
        if len(X.shape) == 2:
            X_scaled = self.scaler_X.transform(X)
            y_scaled = self.scaler_y.transform(y)
            X_seq, y_seq = self.create_sequences(X_scaled, y_scaled)
        else:
            X_seq = X
            y_seq = y
        
        # Prediction
        y_pred_scaled = self.model.predict(X_seq, verbose=0)
        
        # Inverse transform
        y_pred = self.scaler_y.inverse_transform(y_pred_scaled)
        y_true = self.scaler_y.inverse_transform(y_seq)
        
        # Metrics
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
        
        # MAPE (Mean Absolute Percentage Error)
        mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
        
        metrics = {
            'mse': float(mse),
            'rmse': float(rmse),
            'mae': float(mae),
            'r2': float(r2),
            'mape': float(mape)
        }
        
        self.metrics = metrics
        
        logger.info("evaluation_complete",
                   rmse=rmse,
                   mae=mae,
                   r2=r2,
                   mape=mape)
        
        return metrics
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """
        Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù‚ÛŒÙ…Øª
        
        Args:
            X: Features DataFrame
            
        Returns:
            Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù‚ÛŒÙ…Øª
        """
        logger.info("making_prediction", X_shape=X.shape)
        
        # Scaling
        X_scaled = self.scaler_X.transform(X.values)
        
        # Ø§Ú¯Ø± X Ú©Ù…ØªØ± Ø§Ø² sequence_length Ø¨Ø§Ø´Ù‡ØŒ Ù†Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒÙ… predict Ú©Ù†ÛŒÙ…
        if len(X_scaled) < self.sequence_length:
            logger.warning("insufficient_data_for_prediction",
                         required=self.sequence_length,
                         got=len(X_scaled))
            return None
        
        # Ø¢Ø®Ø±ÛŒÙ† sequence
        X_seq = X_scaled[-self.sequence_length:].reshape(1, self.sequence_length, -1)
        
        # Prediction
        y_pred_scaled = self.model.predict(X_seq, verbose=0)
        y_pred = self.scaler_y.inverse_transform(y_pred_scaled)
        
        logger.info("prediction_made", predicted_price=y_pred[0][0])
        
        return y_pred[0][0]
    
    def predict_future(
        self, 
        X: pd.DataFrame, 
        days: int = 7
    ) -> np.ndarray:
        """
        Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú†Ù†Ø¯ Ø±ÙˆØ² Ø¢ÛŒÙ†Ø¯Ù‡
        
        Args:
            X: Features DataFrame (Ø¢Ø®Ø±ÛŒÙ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§)
            days: ØªØ¹Ø¯Ø§Ø¯ Ø±ÙˆØ²Ù‡Ø§ÛŒ Ø¢ÛŒÙ†Ø¯Ù‡
            
        Returns:
            Ø¢Ø±Ø§ÛŒÙ‡ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§
        """
        logger.info("predicting_future", days=days)
        
        predictions = []
        
        for day in range(days):
            # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙˆØ² Ø¨Ø¹Ø¯
            pred = self.predict(X)
            predictions.append(pred)
            
            # Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙˆØ² Ø¨Ø¹Ø¯ÛŒØŒ Ø¨Ø§ÛŒØ¯ Ø§Ø² prediction Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ…
            # (Ø¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø± ÙÙ‚Ø· ÛŒÚ© Ø±ÙˆØ² Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…)
            # Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù…ÛŒâ€ŒØªÙˆÙ†Ù‡ Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ± Ø¨Ø´Ù‡
            
        return np.array(predictions)
    
    def save_model(self, path: str = 'models/lstm_gold_predictor'):
        """
        Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§Øª
        
        Args:
            path: Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡
        """
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        # Ø°Ø®ÛŒØ±Ù‡ Keras model
        self.model.save(f'{path}.h5')
        
        # Ø°Ø®ÛŒØ±Ù‡ scalers
        joblib.dump(self.scaler_X, f'{path}_scaler_X.pkl')
        joblib.dump(self.scaler_y, f'{path}_scaler_y.pkl')
        
        # Ø°Ø®ÛŒØ±Ù‡ config
        config = {
            'sequence_length': self.sequence_length,
            'prediction_horizon': self.prediction_horizon,
            'lstm_units': self.lstm_units,
            'dropout_rate': self.dropout_rate,
            'feature_names': self.feature_names,
            'metrics': self.metrics,
            'training_history': self.training_history
        }
        
        with open(f'{path}_config.json', 'w') as f:
            json.dump(config, f, indent=2)
        
        logger.info("model_saved", path=path)
    
    def load_model_weights(self, path: str = 'models/lstm_gold_predictor'):
        """
        Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
        
        Args:
            path: Ù…Ø³ÛŒØ± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ
        """
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Keras model
        self.model = load_model(f'{path}.h5')
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ scalers
        self.scaler_X = joblib.load(f'{path}_scaler_X.pkl')
        self.scaler_y = joblib.load(f'{path}_scaler_y.pkl')
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ config
        with open(f'{path}_config.json', 'r') as f:
            config = json.load(f)
        
        self.sequence_length = config['sequence_length']
        self.prediction_horizon = config['prediction_horizon']
        self.lstm_units = config['lstm_units']
        self.dropout_rate = config['dropout_rate']
        self.feature_names = config['feature_names']
        self.metrics = config['metrics']
        self.training_history = config['training_history']
        
        logger.info("model_loaded", path=path)
    
    def plot_training_history(self):
        """
        Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø± training history
        """
        import matplotlib.pyplot as plt
        
        if self.training_history is None:
            logger.warning("no_training_history")
            return
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # Loss
        ax1.plot(self.training_history['loss'], label='Training Loss')
        ax1.plot(self.training_history['val_loss'], label='Validation Loss')
        ax1.set_title('Model Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss (MSE)')
        ax1.legend()
        ax1.grid(True)
        
        # MAE
        ax2.plot(self.training_history['mae'], label='Training MAE')
        ax2.plot(self.training_history['val_mae'], label='Validation MAE')
        ax2.set_title('Model MAE')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('MAE')
        ax2.legend()
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig('models/training_history.png', dpi=300)
        plt.close()
        
        logger.info("training_history_plotted")


if __name__ == "__main__":
    print("\n" + "="*70)
    print("ğŸ§  LSTM Gold Price Predictor - Training")
    print("="*70 + "\n")
    
    # 1. Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡
    print("ğŸ“Š Step 1: Preparing data...")
    DATABASE_URL = "postgresql+psycopg2://admin:admin123@localhost:5432/gold_analyzer"
    
    feature_service = FeatureEngineeringService(DATABASE_URL)
    X, y = feature_service.prepare_ml_dataset(prediction_horizon=1)
    
    print(f"âœ… Data ready: X{X.shape}, y{y.shape}")
    
    # 2. Ø³Ø§Ø®Øª Ù…Ø¯Ù„
    print("\nğŸ§  Step 2: Creating LSTM model...")
    model = LSTMGoldPricePredictor(
        sequence_length=60,
        prediction_horizon=1,
        lstm_units=[128, 64, 32],
        dropout_rate=0.2
    )
    
    # 3. Ø¢Ù…ÙˆØ²Ø´
    print("\nğŸš€ Step 3: Training model...")
    print("â±ï¸  This may take 10-30 minutes...")
    
    results = model.train(
        X, y,
        validation_split=0.2,
        epochs=100,
        batch_size=32
    )
    
    # 4. Ù†ØªØ§ÛŒØ¬
    print("\n" + "="*70)
    print("ğŸ“Š Training Results:")
    print("="*70)
    print(f"\nMetrics:")
    for metric, value in results['metrics'].items():
        print(f"  {metric}: {value:.4f}")
    
    print(f"\nSamples:")
    print(f"  Train: {results['train_samples']}")
    print(f"  Validation: {results['val_samples']}")
    
    # 5. Ø°Ø®ÛŒØ±Ù‡
    print("\nğŸ’¾ Saving model...")
    model.save_model('models/lstm_gold_predictor')
    
    # 6. Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø±
    print("ğŸ“Š Plotting training history...")
    model.plot_training_history()
    
    print("\n" + "="*70)
    print("âœ… Training Complete!")
    print("="*70 + "\n")